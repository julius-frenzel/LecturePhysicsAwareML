





!git clone https://github.com/CPShub/LecturePhysicsAwareML.git





import tensorflow as tf
import datetime
now = datetime.datetime.now
import LecturePhysicsAwareML.FFNN_introduction.data as ld
import LecturePhysicsAwareML.FFNN_introduction.models as lm
import LecturePhysicsAwareML.FFNN_introduction.plots as lp





import tensorflow as tf
import datetime
now = datetime.datetime.now
import data as ld
import models as lm
import plots as lp





%rm -rf LecturePhysicsAwareML





# Adapt the model name for your plots
model_name = 'FFNN'

# Number of nodes in each layer
units = [32,32,1]

# Activation function in each layer
# Options: 'softplus', 'tanh', 'relu', 'linear', ...
activation = ['softplus','softplus','linear']

# Load model
model = lm.main(units=units, activation=activation)

# Dataset options: 'bathtub', 'curve', 'double_curve'
data = 'curve'

# Epochs: number of iterations in the optimisation process
epochs = 1000

# Load data
xs, ys, xs_c, ys_c = ld.get_data(data)

# Calibrate model
t1 = now()
print(t1)

# set "verbose=2" to observe the progress of the calibration process
model.optimizer.learning_rate.assign(0.002)
h = model.fit([xs_c], [ys_c], epochs = epochs,  verbose = 0)

t2 = now()
print('it took', t2 - t1, '(sec) to calibrate the model')

# Plot loss and prediction
lp.plot_loss(h)
lp.plot_data_model(xs, ys, xs_c, ys_c, model, model_name, data, 4)





# Adapt the model name for your plots
model_name = 'FFNN'

# Number of nodes in each layer
units = [32,32,1]

# Activation function in each layer
# Options: 'softplus', 'tanh', 'relu', 'linear', ...
activation = ['softplus','softplus','linear']

# non_neg: restrict the weights in different layers to be non-negative
non_neg = [False, True, False]

# Dataset options: 'bathtub', 'curve', 'double_curve'
data = 'curve'

# epochs: number of iterations in the optimisation process
epochs = 3000

 # Load model
model = lm.main_con(units=units, activation=activation, non_neg=non_neg)

# Load data
xs, ys, xs_c, ys_c = ld.get_data(data)

# Calibrate model
t1 = now()
print(t1)

#   set "verbose=2" to observe the progress of the calibration process
model.optimizer.learning_rate.assign(0.002)
h = model.fit([xs_c], [ys_c], epochs = epochs,  verbose = 0)

t2 = now()
print('it took', t2 - t1, '(sec) to calibrate the model')

# Plot loss and prediction
lp.plot_loss(h)
lp.plot_data_model(xs, ys, xs_c, ys_c, model, model_name, data, 4)





# Adapt the model name for your plots
model_name = 'Sobolev'

# Number of nodes in each layer
units = [32,32,1]

# Activation function in each layer
# Options: 'softplus', 'tanh', 'relu', 'linear', ...
activation = ['tanh','softplus','linear']

# non_neg: restrict the weights in different layers to be non-negative
non_neg = [True, True, True]

# Dataset options: 'bathtub', 'curve', 'double_curve'
data = 'double_curve'

# epochs: number of iterations in the optimisation process
epochs = 3000

# Set loss_weights to define the calibration process...
# ...[1,0] calibration only on the function
# ...[0,1] calibration only on the gradient
# ...[1,1] calibration on the function and its gradient
loss_weights=[0,1]

# Load model
model = lm.main_con_grad(loss_weights=loss_weights, \
                         units=units, activation=activation, non_neg=non_neg)

# Load data
xs, ys, dys, xs_c, ys_c, dys_c = ld.get_data_with_gradients(data)

# Calibrate model
t1 = now()
print(t1)

# set "verbose=2" to observe the progress of the calibration process
model.optimizer.learning_rate.assign(0.002)
h = model.fit([xs_c], [ys_c, dys_c], epochs = epochs,  verbose = 0)

t2 = now()
print('it took', t2 - t1, '(sec) to calibrate the model')

# Plot loss and prediction
lp.plot_loss(h)
lp.plot_data_model_grad(xs, ys, dys, xs_c, ys_c, dys_c, model, model_name, data, 4)
