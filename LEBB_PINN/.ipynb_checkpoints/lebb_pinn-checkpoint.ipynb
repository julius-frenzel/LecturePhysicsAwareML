{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529eb88b-52a2-43dd-9449-c76609f3d7c7",
   "metadata": {},
   "source": [
    "Equations for transient Euler-Bernoulli beam:\n",
    "\n",
    "$$\\mu \\frac{d^2w}{dt^2} + \\frac{d^2}{dx^2} \\left( EI \\frac{d^2w}{dx^2} \\right) = q$$\n",
    "\n",
    "$$-EI \\frac{d^2w}{dx^2} = M$$\n",
    "\n",
    "$$-\\frac{d}{dx} \\left( EI \\frac{d^2w}{dx^2} \\right) = Q$$\n",
    "\n",
    "$E$: Young's modulus \\\n",
    "$I$: moment of inertia \\\n",
    "$\\mu$: linear mass density\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd54ef-4375-465d-af59-204094c44fe4",
   "metadata": {},
   "source": [
    "## Data Driven Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afa893-12c7-41a3-a191-2558be55b0a6",
   "metadata": {},
   "source": [
    "define a class for the FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa5562-2972-4f51-ae45-0f990b202503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output, update_display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "class FFNN(torch.nn.Module):\n",
    "    def __init__(self, layers, nonlinearity=\"sigmoid\"):\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.dtype = torch.float64\n",
    "        \n",
    "        self.layers = nn.ParameterDict({f\"layer_{i}\": nn.Linear(layer_i, layer_ip1, bias=(i < len(layers_widths) - 2), dtype=self.dtype) for i, (layer_i, layer_ip1) in enumerate(zip(layers_widths[:-1], layers_widths[1:]))})\n",
    "\n",
    "        if nonlinearity == \"sigmoid\":\n",
    "            self.nonlinearity = nn.Sigmoid()\n",
    "        elif nonlinearity == \"tanh\":\n",
    "            self.nonlinearity = nn.Tanh()\n",
    "        elif nonlinearity == \"relu\":\n",
    "            self.nonlinearity = nn.ReLU()\n",
    "        elif nonlinearity == \"leaky_relu\":\n",
    "            self.nonlinearity = nn.LeakyReLU()\n",
    "        elif nonlinearity == \"softplus\":\n",
    "            self.nonlinearity = nn.Softplus()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid nonlinearity.\")\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(f\"shape of x: {x.shape}\")\n",
    "        y = x\n",
    "        for layer_index, layer_name in enumerate(self.layers):\n",
    "            y = self.layers[layer_name](y)\n",
    "            #print(f\"shape of y: {y.shape}\")\n",
    "            # apply nonlinearity for all but the last layer\n",
    "            if layer_index < len(self.layers) - 1:\n",
    "                y = self.nonlinearity(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92521d25-00d4-4d03-b9d0-e6d62121c155",
   "metadata": {},
   "source": [
    "definition of the beam model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad853993-6cfd-4346-8c42-f8a352d9c85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "l = 1 # length of the beam (left end is at x=0)\n",
    "E = lambda t,x: 1 # Young's modulus\n",
    "I = lambda t,x: 1 # moment of inertia\n",
    "mu = lambda t,x: 1 # linear mass density\n",
    "\n",
    "dynamic = True # whether the simulation should include dnamic effects\n",
    "t_end = 0.1 if dynamic else 0 # time, at which the simulation ends (start time is t=0)\n",
    "\n",
    "n_samples_t = 3 if dynamic else 1\n",
    "n_samples_x = 20\n",
    "\n",
    "# boundary conditions\n",
    "# clamped --> (w, w_x)\n",
    "# free --> (Q, M)\n",
    "# pinned --> (w, M)\n",
    "bcs = {\"left boundary\": {\"x\": 0, \"type\": \"clamped\", \"values\": (0., 0.)},\n",
    "       \"right boundary\": {\"x\": 1, \"type\": \"free\", \"values\": (0., 0.)}}\n",
    "q = lambda x: 0.\n",
    "\n",
    "# initial conditions\n",
    "ics = {\"w\": lambda x: 3/10*1/6*(3*x**2 - x**3 + (x-1)**3), \"w_t\": lambda x: 0.}\n",
    "\n",
    "#def batched_grad(inputs, outputs, create_graph):\n",
    "#    print(f\"input shape for batched gradients: {inputs.shape}\")\n",
    "#    print(inputs[0])\n",
    "#    print(outputs[0])\n",
    "#    return torch.stack([torch.autograd.grad(inputs=inputs[i], outputs=outputs[i], create_graph=create_graph) for i in range(len(inputs))])\n",
    "\n",
    "def comp_loss(t, x, w):\n",
    "    batch_shape = t.shape\n",
    "    loss = {key: torch.tensor(0.) for key in [\"pde\", \"w\", \"w_x\", \"Q\", \"M\", \"w_i\", \"w_t_i\"]}\n",
    "\n",
    "    # pde\n",
    "    w_x = torch.autograd.grad(inputs=x, outputs=w, grad_outputs=torch.ones(batch_shape), create_graph=True, retain_graph=True)[0]\n",
    "    w_xx = torch.autograd.grad(inputs=x, outputs=w_x, grad_outputs=torch.ones(batch_shape), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    eiw_xx = E(t,x)*I(t,x)*w_xx\n",
    "    eiw_xx_x = torch.autograd.grad(inputs=x, outputs=eiw_xx, grad_outputs=torch.ones(batch_shape), create_graph=True, retain_graph=True)[0]\n",
    "    eiw_xx_xx = torch.autograd.grad(inputs=x, outputs=eiw_xx_x, grad_outputs=torch.ones(batch_shape), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    w_t = torch.autograd.grad(inputs=t, outputs=w, grad_outputs=torch.ones(batch_shape), create_graph=True, retain_graph=True)[0]\n",
    "    w_tt = torch.autograd.grad(inputs=t, outputs=w_t, grad_outputs=torch.ones(batch_shape), create_graph=True, retain_graph=True)[0]\n",
    "    muw_tt = mu(t,x)*w_tt\n",
    "\n",
    "    loss[\"pde\"] = ((muw_tt*dynamic + eiw_xx_xx - q(x))**2).mean()\n",
    "\n",
    "    # boundary conditions\n",
    "    for bc_name in bcs:\n",
    "        bc = bcs[bc_name]\n",
    "\n",
    "        # initial conditions take precedence in the dynamic case\n",
    "        if dynamic:\n",
    "            boundary_mask = torch.logical_and(x == bc[\"x\"], (t > 0))\n",
    "        else:\n",
    "            boundary_mask = x == bc[\"x\"]\n",
    "\n",
    "        if boundary_mask.sum() > 0:\n",
    "            if bc[\"type\"] == \"clamped\":\n",
    "                loss[\"w\"] = loss[\"w\"] + ((w[boundary_mask] - bc[\"values\"][0])**2).mean() # w\n",
    "                loss[\"w_x\"] = loss[\"w_x\"] + ((w_x[boundary_mask] - bc[\"values\"][1])**2).mean() # w_x\n",
    "    \n",
    "            elif bc[\"type\"] == \"free\":\n",
    "                loss[\"Q\"] = loss[\"Q\"] + ((-eiw_xx_x[boundary_mask] - bc[\"values\"][0])**2).mean() # Q\n",
    "                loss[\"M\"] = loss[\"M\"] + ((-eiw_xx[boundary_mask] - bc[\"values\"][1])**2).mean() # M\n",
    "\n",
    "            elif bc[\"type\"] == \"pinned\":\n",
    "                loss[\"w\"] = loss[\"w\"] + ((w[boundary_mask] - bc[\"values\"][0])**2).mean() # w\n",
    "                loss[\"M\"] = loss[\"M\"] + ((-eiw_xx[boundary_mask] - bc[\"values\"][1])**2).mean() # M\n",
    "            else:\n",
    "                raise ValueError(f\"invalid type of boundary condition: {bc['type']}\")\n",
    "\n",
    "    # initial conditions\n",
    "    if dynamic:\n",
    "        ic_mask = t == 0.\n",
    "        #print(x[ic_mask])\n",
    "        #print(ics[\"w\"](x[ic_mask]))\n",
    "        if ic_mask.sum() > 0:\n",
    "            loss[\"w_i\"] = loss[\"w_i\"] + ((w[ic_mask] - ics[\"w\"](x[ic_mask]))**2).mean() # w\n",
    "            loss[\"w_t_i\"] = loss[\"w_t_i\"] + ((w_t[ic_mask] - ics[\"w_t\"](x[ic_mask]))**2).mean() # w_t\n",
    "    \n",
    "    \n",
    "    return loss\n",
    "\n",
    "meshgrid = torch.meshgrid(torch.linspace(0, t_end, n_samples_t),\n",
    "                           torch.linspace(0, l, n_samples_x))\n",
    "points_t = meshgrid[0].reshape(-1,1)\n",
    "points_x = meshgrid[1].reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809fe08b-f099-44ca-80d5-ab15c56de45c",
   "metadata": {},
   "source": [
    "class for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6204eb-c5b1-432c-af63-ff86edc4a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualization:\n",
    "    def __init__(self, model, l, t_end, plot_interval=10, dynamic=True):\n",
    "        self.model=model\n",
    "        self.l = l\n",
    "        self.t_end = t_end\n",
    "        self.plot_interval = plot_interval\n",
    "        self.dynamic = dynamic\n",
    "        self.deflection_limits = None\n",
    "\n",
    "        self.fig = self.axs = None\n",
    "        self.vis_data = None\n",
    "        self.figsize = (8, 10)\n",
    "\n",
    "    \n",
    "    def vis(self, vis_data):\n",
    "        vis_data = {plot_key: {key: [value] for key, value in zip(plot_data.keys(), plot_data.values())} for plot_key, plot_data in zip(vis_data.keys(), vis_data.values())}\n",
    "        if self.vis_data is None or not len(vis_data) == len(self.vis_data):\n",
    "            self.vis_data = vis_data\n",
    "            self.fig, self.axs = plt.subplots(len(vis_data) + 1, 1, figsize=self.figsize)\n",
    "            self.display_id = 'display_id'+str(random.random())\n",
    "            display(self.fig, display_id=self.display_id)\n",
    "        else:\n",
    "            for plot_key in vis_data:\n",
    "                for data_field in vis_data[plot_key]:\n",
    "                    self.vis_data[plot_key][data_field] += vis_data[plot_key][data_field]\n",
    "\n",
    "        if (iter_index + 1) % self.plot_interval == 0:\n",
    "            for plot_index, plot_key in enumerate(self.vis_data):\n",
    "\n",
    "                title = plot_key\n",
    "                data_x = list(self.vis_data[plot_key].values())[0]\n",
    "                data_y = list(self.vis_data[plot_key].values())[1]\n",
    "                label_x = list(self.vis_data[plot_key].keys())[0]\n",
    "                label_y = list(self.vis_data[plot_key].keys())[1]\n",
    "                self.axs[plot_index].clear()\n",
    "                self.axs[plot_index].plot(data_x, data_y)\n",
    "                self.axs[plot_index].set_xlabel(label_x)\n",
    "                self.axs[plot_index].set_ylabel(label_y)\n",
    "                self.axs[plot_index].set_yscale(\"log\" if \"loss\" in label_y else \"linear\")\n",
    "                self.axs[plot_index].set_title(title)\n",
    "                self.axs[plot_index].grid(True)\n",
    "\n",
    "                plot_index = len(self.vis_data)\n",
    "                self.axs[plot_index].clear()\n",
    "                if self.deflection_limits is not None:\n",
    "                    deflection_delta = self.deflection_limits[1] - self.deflection_limits[0]\n",
    "                    self.deflection_limits[0] = self.deflection_limits[0] + deflection_delta*0.01\n",
    "                    self.deflection_limits[1] = self.deflection_limits[1] - deflection_delta*0.01\n",
    "            for t_i in np.linspace(0, self.t_end, 3 if self.dynamic else 1):\n",
    "                x_eval = torch.linspace(0, self.l, 100, dtype=self.model.dtype).reshape(-1,1)\n",
    "                t_eval = t_i*torch.ones_like(x_eval, dtype=self.model.dtype).reshape(-1,1)\n",
    "                self.model.eval()\n",
    "                w_eval = self.model(torch.concatenate([t_eval, x_eval], dim=1)).detach()\n",
    "\n",
    "                if self.deflection_limits is None:\n",
    "                    self.deflection_limits = [w_eval.min(), w_eval.max()]\n",
    "                else:\n",
    "                    self.deflection_limits[0] = min(w_eval.min(), self.deflection_limits[0])\n",
    "                    self.deflection_limits[1] = max(w_eval.max(), self.deflection_limits[1])\n",
    "    \n",
    "                self.axs[plot_index].plot(x_eval, w_eval, label=f\"t={t_i:.2f}\")\n",
    "                \n",
    "            self.axs[plot_index].set_xlabel(\"x in m\")\n",
    "            self.axs[plot_index].set_ylabel(\"w in m\")\n",
    "            deflection_delta = self.deflection_limits[1] - self.deflection_limits[0]\n",
    "            self.axs[plot_index].set_ylim([self.deflection_limits[0] - 0.1*deflection_delta, self.deflection_limits[1] + 0.1*deflection_delta])\n",
    "            self.axs[plot_index].set_title(\"deflection for different points in time\")\n",
    "            self.axs[plot_index].grid(True)\n",
    "            if self.dynamic:\n",
    "                self.axs[plot_index].legend()\n",
    "\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            update_display(self.fig, display_id=self.display_id)\n",
    "            #plt.savefig('training_plot_cnn.svg', format='svg')\n",
    "            #plt.pause(0.1)\n",
    "            \n",
    "            #print(f\"w_x_mean_actual: {w_eval[x_eval == x_eval.max()].mean() - w_eval[x_eval == x_eval.min()].mean()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf43c1c5-6725-459f-81d5-758a003dfe72",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20153cb4-159a-46eb-bf34-07bd0b5c87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # ensure reproducibility\n",
    "\n",
    "layers_widths = [2, 50, 50, 50, 1]\n",
    "\n",
    "ffnn = FFNN(layers_widths, nonlinearity=\"tanh\")\n",
    "\n",
    "print(ffnn.layers)\n",
    "#print(list(ffnn.layers[\"layer_0\"].parameters())[0])\n",
    "#print(list(ffnn.layers[\"layer_1\"].parameters())[0])\n",
    "#print(list(ffnn.layers[\"layer_2\"].parameters())[0])\n",
    "#time.sleep(100)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=ffnn.parameters(), lr=0.005, weight_decay=0)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1.)\n",
    "\n",
    "visualization = Visualization(model=ffnn, l=l, t_end=t_end, plot_interval=50, dynamic=dynamic)\n",
    "\n",
    "x = points_x.type(ffnn.dtype).requires_grad_()\n",
    "t = (points_t).type(ffnn.dtype).requires_grad_()\n",
    "\n",
    "#print(t)\n",
    "\n",
    "batch_size = n_samples_t*n_samples_x - 11\n",
    "def get_batch(data, iter_index, batch_size):\n",
    "    data_len = len(data[0])\n",
    "    start_index = (iter_index*batch_size) % data_len\n",
    "    stop_index = ((iter_index + 1)*batch_size) % data_len\n",
    "\n",
    "    batch_data = []\n",
    "    for input in data:\n",
    "        if start_index < stop_index:\n",
    "            batch_data.append(input[start_index:stop_index])\n",
    "        else:\n",
    "            batch_data.append(torch.concatenate([input[start_index:], input[:stop_index]], dim=0))\n",
    "\n",
    "    return tuple(batch_data)\n",
    "\n",
    "maxiter = np.inf\n",
    "        \n",
    "iter_index = 0\n",
    "scheduler_index = 0\n",
    "while True:\n",
    "    #print(f\"iteration {iter_index + 1}\")\n",
    "    ffnn.zero_grad()\n",
    "    ffnn.train()\n",
    "\n",
    "    t_i, x_i = get_batch([t, x], iter_index, batch_size)\n",
    "    #t_i = t[iter_index % len(t)].reshape(1,1)\n",
    "    #x_i = x[iter_index % len(t)].reshape(1,1)\n",
    "    w = ffnn(torch.concatenate([t_i, x_i], dim=1))\n",
    "\n",
    "    #w_x = torch.autograd.grad(inputs=x_i, outputs=w, create_graph=True, retain_graph=True)[0]\n",
    "    #print(f\"x_i: {x_i}\")\n",
    "    #print(f\"w: {w}\")\n",
    "    #print(f\"w_x: {w_x}\")\n",
    "    \n",
    "    losses = comp_loss(t_i, x_i, w)\n",
    "    \n",
    "    loss_sum = sum(losses.values()) # !!!!!!!!!!!!!!!!!make it so the losses have the same orders of magnitude?!!!!!!!!!!!!!!!!\n",
    "    loss_normalized = 0.\n",
    "    #norm_value = torch.concatenate([v.reshape(1,1) for v in losses.values()]).max()\n",
    "    #print(f\"max loss: {norm_value}\")\n",
    "    tolerances = {\"pde\": 1e-2, \"w\": 1e-2, \"w_x\": 1e-2, \"Q\": 1e-2, \"M\": 1e-2, \"w_i\": 1e-5, \"w_t_i\": 1e-4}\n",
    "    for loss_key, loss_comp in zip(losses.keys(), losses.values()):\n",
    "        if loss_comp > 0:\n",
    "            loss_normalized = loss_normalized + loss_comp /(loss_comp.detach() + tolerances[loss_key])\n",
    "\n",
    "    losses[\"pde\"] = 1.*losses[\"pde\"]\n",
    "    losses[\"w\"] = 1.*losses[\"w\"]\n",
    "    losses[\"w_x\"] = 1.*losses[\"w_x\"]\n",
    "    losses[\"Q\"] = 10.*losses[\"Q\"]\n",
    "    losses[\"M\"] = 1.*losses[\"M\"]\n",
    "    losses[\"w_i\"] = 1e2*losses[\"w_i\"]\n",
    "    losses[\"w_t_i\"] = 1.*losses[\"w_t_i\"]\n",
    "\n",
    "    loss_normalized = sum(losses.values())\n",
    "    #print(f\"loss: {loss}\")\n",
    "    loss_normalized.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #print(f\"gradient: {list(ffnn.layers['layer_1'].parameters())[0].grad}\")\n",
    "\n",
    "    #print(f\"loss: {loss}\")\n",
    "    #print(iter_index)\n",
    "    #print(loss)\n",
    "    vis_data = {\"total loss over iterations\": {\"iterations\": iter_index+1, \"total loss\": loss_normalized.detach()},\n",
    "                \"PDE-loss over iterations\": {\"iterations\": iter_index+1, \"PDE-loss\": losses[\"pde\"].detach()},\n",
    "                \"w_i-loss over iterations\": {\"iterations\": iter_index+1, \"w_i-loss\": losses[\"w_i\"].detach()},\n",
    "                \"Q-loss over iterations\": {\"iterations\": iter_index+1, \"Q-loss\": losses[\"Q\"].detach()}}\n",
    "    visualization.vis(vis_data)\n",
    "\n",
    "    iter_index += 1\n",
    "\n",
    "    scheduler_index += batch_size\n",
    "    if scheduler_index > len(t):\n",
    "        scheduler_index = scheduler_index % len(t)\n",
    "        lr_scheduler.step()\n",
    "        print(f\"new learning rate: {lr_scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "    if iter_index == maxiter:\n",
    "        break\n",
    "\n",
    "#grad_x = torch.autograd.grad(inputs=x, outputs=y, create_graph=True) # create graph for gradients, so that they can be used in the loss function    \n",
    "\n",
    "#print(f\"gradient of x: {grad_x}\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2459646-5b13-487f-a287-c559925f5c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f934e6cf-4244-4e06-97dc-40ccd54c3549",
   "metadata": {},
   "source": [
    "## parameter Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d863d85a-5179-4062-88df-07e8b6cbbc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2786737e-f0e7-43ec-8dc9-803c6e3e2ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aabf85-294a-4339-901c-c25f5d9e94c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887c7f5-957c-4485-b0ea-fae89c855866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0b18e-7681-4142-b88e-8955b9ca5f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d5bb76-c297-4a38-b267-c10a05d6ce4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (paml_env_pytorch)",
   "language": "python",
   "name": "paml_env_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
