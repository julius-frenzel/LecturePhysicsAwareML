














import torch
import warnings
warnings.filterwarnings('ignore', message='Attempting to run cuBLAS, but there was no current CUDA context!') # ignore confusing warning without noteceable consequences

n_devices = torch.cuda.device_count()
print(f"number of available CUDA devices: {n_devices}")
for i in range(n_devices):
    device_name = f"cuda:{i}"
    print(f"\t \"{device_name}\" ({torch.cuda.get_device_name(device=device_name)})")









import torch.nn as nn
import random
import time
import numpy as np
from IPython.display import display, clear_output, update_display
import matplotlib.pyplot as plt
%matplotlib widget

class FFNN(torch.nn.Module):
    def __init__(self, layer_widths, nonlinearity="sigmoid", device="cpu"):
        super(FFNN, self).__init__()

        # select device for the model to run on
        if not torch.cuda.is_available():
            device = "cpu"
        self.device = device
        print(f"using device \"{self.device}\"")

        # set up the layers
        self.layer_widths = layer_widths
        self.dtype = torch.float32
        layers = []
        for i in range(len(layer_widths) - 1):
            layers.append(nn.Linear(layer_widths[i], layer_widths[i + 1], bias=(i < len(layer_widths) - 2)))
            if i < len(layer_widths) - 2:  # apply nonlinearity for all but the last layer
                if nonlinearity == "sigmoid":
                    layers.append(nn.Sigmoid())
                elif nonlinearity == "tanh":
                    layers.append(nn.Tanh())
                elif nonlinearity == "relu":
                    layers.append(nn.ReLU())
                elif nonlinearity == "leaky_relu":
                    layers.append(nn.LeakyReLU())
                elif nonlinearity == "softplus":
                    layers.append(nn.Softplus())
                else:
                    raise ValueError("Invalid nonlinearity.")
        self.network = nn.Sequential(*layers).to(device=self.device, dtype=self.dtype)


    # forward pass through the model
    def forward(self, x):
        x = x.to(self.device, dtype=self.dtype)
        return self.network(x)









# boundary conditions
# clamped --> (w, w_x)
# free --> (Q, M)
# pinned --> (w, M)
bcs = {"left boundary": {"x": 0, "type": "clamped", "values": (0., 0.)},
       "right boundary": {"x": 1, "type": "free", "values": (0., 0.)}}
q = lambda x: 0.

# initial conditions
ics = {"w": lambda x: 1/24*(x**4 - 4*x**3 + 6*x**2), "w_t": lambda x: 0.}

def comp_loss(t, x, w, E, I):
    loss = {key: torch.tensor([], device=t.device) for key in ["pde", "w", "w_x", "Q", "M", "w_i", "w_t_i"]}
    grad_output = torch.ones_like(t, device=x.device)

    # ----------pde----------
    w_x = torch.autograd.grad(inputs=x, outputs=w, grad_outputs=grad_output, create_graph=True, retain_graph=True)[0]
    w_xx = torch.autograd.grad(inputs=x, outputs=w_x, grad_outputs=grad_output, create_graph=True, retain_graph=True)[0]

    general_general_reations = True
    if general_general_reations:
        # general relations for E(x)
        eiw_xx = E(x)*I(x)*w_xx
        eiw_xx_x = torch.autograd.grad(inputs=x, outputs=eiw_xx, grad_outputs=grad_output, create_graph=True, retain_graph=True)[0]
        eiw_xx_xx = torch.autograd.grad(inputs=x, outputs=eiw_xx_x, grad_outputs=grad_output, create_graph=True, retain_graph=True)[0]
    else:
        # simplified relations assuming E=const
        w_xxx = torch.autograd.grad(inputs=x, outputs=w_xx, grad_outputs=grad_output, create_graph=True, retain_graph=True)[0]
        w_xxxx = torch.autograd.grad(inputs=x, outputs=w_xxx, grad_outputs=grad_output, create_graph=True, retain_graph=True)[0]

        eiw_xx = E(x)*I(x)*w_xx
        eiw_xx_x = E(x)*I(x)*w_xxx
        eiw_xx_xx = E(x)*I(x)*w_xxxx

    w_t = torch.autograd.grad(inputs=t, outputs=w, grad_outputs=grad_output, create_graph=True, retain_graph=True)[0]
    w_tt = torch.autograd.grad(inputs=t, outputs=w_t, grad_outputs=grad_output, create_graph=True, retain_graph=True)[0]
    muw_tt = mu(x)*w_tt

    loss["pde"] = (muw_tt*dynamic + eiw_xx_xx - q(x))**2

    # ----------boundary conditions----------
    for bc_name, bc in zip(bcs.keys(), bcs.values()):

        # initial conditions take precedence in the dynamic case
        if dynamic:
            boundary_mask = torch.logical_and(x == bc["x"], t > 0)
        else:
            boundary_mask = x == bc["x"]

        if boundary_mask.sum() > 0:
            if bc["type"] == "clamped":
                loss["w"] = torch.concatenate([loss["w"], (w[boundary_mask] - bc["values"][0])**2]) # w
                loss["w_x"] = torch.concatenate([loss["w_x"], (w_x[boundary_mask] - bc["values"][1])**2]) # w_x
    
            elif bc["type"] == "free":
                loss["Q"] = torch.concatenate([loss["Q"], (-eiw_xx_x[boundary_mask] - bc["values"][0])**2]) # Q
                loss["M"] = torch.concatenate([loss["M"], (-eiw_xx[boundary_mask] - bc["values"][1])**2]) # M

            elif bc["type"] == "pinned":
                loss["w"] = torch.concatenate([loss["w"], (w[boundary_mask] - bc["values"][0])**2]) # w
                loss["M"] = torch.concatenate([loss["M"], (-eiw_xx[boundary_mask] - bc["values"][1])**2]) # M
            else:
                raise ValueError(f"invalid type of boundary condition: {bc['type']}")

    # ----------initial conditions----------
    if dynamic:
        ic_mask = t == 0.
        if ic_mask.sum() > 0:
            loss["w_i"] = torch.concatenate([loss["w_i"] , (w[ic_mask] - ics["w"](x[ic_mask]))**2]) # w
            loss["w_t_i"] = torch.concatenate([loss["w_t_i"], (w_t[ic_mask] - ics["w_t"](x[ic_mask]))**2]) # w_t

    # compute means of all loss components separately
    loss = {key: (l.mean() if len(l) > 0 else torch.tensor(0., dtype=t.dtype, device=t.device)) for key, l in zip(loss.keys(), loss.values())}
    
    return loss







class Visualization:
    def __init__(self, model, l, t_end, plot_interval=10, dynamic=True):
        self.model=model
        self.l = l
        self.t_end = t_end
        self.plot_interval = plot_interval
        self.dynamic = dynamic
        self.deflection_limits = None

        self.fig = self.axs = None
        self.vis_data = None
        self.figsize = (8, 12)

    
    def vis(self, vis_data):
        vis_data = {plot_key: {key: [value] for key, value in zip(plot_data.keys(), plot_data.values())} for plot_key, plot_data in zip(vis_data.keys(), vis_data.values())}
        if self.vis_data is None or not len(vis_data) == len(self.vis_data):
            self.vis_data = vis_data
            self.fig, self.axs = plt.subplots(len(vis_data) + 1, 1, figsize=self.figsize)
            self.display_id = 'display_id'+str(random.random())
            display(self.fig, display_id=self.display_id)
        else:
            for plot_key in vis_data:
                for data_field in vis_data[plot_key]:
                    self.vis_data[plot_key][data_field] += vis_data[plot_key][data_field]

        if (iter_index + 1) % self.plot_interval == 0:
            for plot_index, plot_key in enumerate(self.vis_data):

                title = plot_key
                data_x = list(self.vis_data[plot_key].values())[0]
                data_y = list(self.vis_data[plot_key].values())[1]
                label_x = list(self.vis_data[plot_key].keys())[0]
                label_y = list(self.vis_data[plot_key].keys())[1]
                self.axs[plot_index].clear()
                self.axs[plot_index].plot(data_x, data_y)
                self.axs[plot_index].set_xlabel(label_x)
                self.axs[plot_index].set_ylabel(label_y)
                self.axs[plot_index].set_yscale("log" if "loss" in label_y or "learning rate" in label_y else "linear")
                self.axs[plot_index].set_title(title)
                self.axs[plot_index].grid(True)

                plot_index = len(self.vis_data)
                self.axs[plot_index].clear()
                if self.deflection_limits is not None:
                    deflection_delta = self.deflection_limits[1] - self.deflection_limits[0]
                    self.deflection_limits[0] = self.deflection_limits[0] + deflection_delta*0.01
                    self.deflection_limits[1] = self.deflection_limits[1] - deflection_delta*0.01
            for t_i in np.linspace(0, self.t_end, 3 if self.dynamic else 1):
                x_eval = torch.linspace(0, self.l, 100, dtype=self.model.dtype).reshape(-1,1)
                t_eval = t_i*torch.ones_like(x_eval, dtype=self.model.dtype).reshape(-1,1)
                self.model.eval()
                w_eval = self.model(torch.concatenate([t_eval, x_eval], dim=1)).detach().to("cpu")[:,0].reshape(-1,1)

                if self.deflection_limits is None:
                    self.deflection_limits = [w_eval.min(), w_eval.max()]
                else:
                    self.deflection_limits[0] = min(w_eval.min(), self.deflection_limits[0])
                    self.deflection_limits[1] = max(w_eval.max(), self.deflection_limits[1])

                self.axs[plot_index].plot(x_eval, w_eval, label=f"t={t_i:.2f}")
                
            self.axs[plot_index].set_xlabel("x in m")
            self.axs[plot_index].set_ylabel("w in m")
            deflection_delta = self.deflection_limits[1] - self.deflection_limits[0]
            self.axs[plot_index].set_ylim([self.deflection_limits[0] - 0.1*deflection_delta, self.deflection_limits[1] + 0.1*deflection_delta])
            self.axs[plot_index].set_title("deflection for different points in time")
            self.axs[plot_index].grid(True)
            if self.dynamic:
                self.axs[plot_index].legend()

            
            plt.tight_layout()
            update_display(self.fig, display_id=self.display_id)
            #plt.savefig('training_plot_cnn.svg', format='svg')
            #plt.pause(0.1)
            
            #print(f"w_x_mean_actual: {w_eval[x_eval == x_eval.max()].mean() - w_eval[x_eval == x_eval.min()].mean()}")







def get_batch(data, iter_index, batch_size, random_samples=False):
    data_len = len(data[0])
    if random_samples:
        batch_indices = torch.randperm(data_len)[:batch_size]
    else:
        start_index = (iter_index*batch_size) % data_len
        stop_index = ((iter_index + 1)*batch_size) % data_len
        batch_indices = np.arange(start_index, stop_index)
        if start_index > stop_index:
            start_index -= data_len
        batch_indices = np.arange(start_index, stop_index)

    batch_data = []
    for input in data:
        batch_data.append(input[batch_indices].reshape(-1,1))

    return tuple(batch_data)





torch.manual_seed(0) # ensure reproducibility

# -----------problem definition----------
# parameters
l = 1. # length of the beam (left end is at x=0)
E = lambda x: 1. # Young's modulus
I = lambda x: 1. # moment of inertia
mu = lambda x: 1. # linear mass density

# line load
q = lambda x: 1.

dynamic = False # whether the simulation should include dnamic effects
t_end = 0.1 if dynamic else 0 # time, at which the simulation ends (start time is t=0)

# ----------setup of the model and optimizer----------
layer_widths_widths = [2, 200, 200, 1]

ffnn = FFNN(layer_widths_widths, nonlinearity="tanh", device="cuda:0")
print(ffnn.network)

optimizer = torch.optim.NAdam(params=ffnn.parameters(), lr=0.001, weight_decay=0)
lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1)

# ----------setup of the visualization----------
visualization = Visualization(model=ffnn, l=l, t_end=t_end, plot_interval=50, dynamic=dynamic)

# ----------setup of the points, at which the physics loss will be evaluated----------
n_samples_t = 5 if dynamic else 1
n_samples_x = 1000
meshgrid = torch.meshgrid(torch.linspace(0, t_end, n_samples_t),
                           torch.linspace(0, l, n_samples_x), indexing="ij")
t = meshgrid[0].reshape(-1,1).type(ffnn.dtype).requires_grad_().to(ffnn.device)
x = meshgrid[1].reshape(-1,1).type(ffnn.dtype).requires_grad_().to(ffnn.device)


# ----------training loop----------
batch_size = n_samples_t*n_samples_x # can be any value between 1 and n_samples_t*n_samples_x
maxiter = 20000 # maximmum number of iterations before the training is stopped
iter_index = 0
scheduler_index = 0
while True:
    ffnn.zero_grad()
    ffnn.train()

    # retrieve batch
    t_i, x_i = get_batch([t, x], iter_index, batch_size, random_samples=True)

    # forward pass
    w = ffnn(torch.concatenate([t_i, x_i], dim=1))

    # compute components of physics loss
    losses = comp_loss(t_i, x_i, w, E=E, I=I)

    # weighting of loss components
    losses["pde"] = 1.*losses["pde"]
    losses["w"] = 5.*losses["w"]
    losses["w_x"] = 1.*losses["w_x"]
    losses["Q"] = 1.*losses["Q"]
    losses["M"] = 1.*losses["M"]
    losses["w_i"] = 2.*losses["w_i"]
    losses["w_t_i"] = 1.*losses["w_t_i"]

    loss_normalized = sum(losses.values())

    # backpropagation
    loss_normalized.backward()
    
    # optimizer step
    optimizer.step()

    # learning rate scheduling (one step per epoch)
    scheduler_index += batch_size
    if scheduler_index > len(t):
        scheduler_index = scheduler_index % len(t)
        lr_scheduler.step()

    # visualization
    losses = {key: value.to("cpu") for key, value in zip(losses.keys(), losses.values())}
    loss_normalized = loss_normalized.to("cpu")
    vis_data = {"total loss over iterations": {"iterations": iter_index+1, "total loss": loss_normalized.detach()},
                "learning rate over iterations": {"iterations": iter_index+1, "learning rate": lr_scheduler.get_last_lr()},
                "PDE-loss over iterations": {"iterations": iter_index+1, "PDE-loss": losses["pde"].detach()},
                "w-loss over iterations": {"iterations": iter_index+1, "w-loss": losses["w"].detach()},
                "w_i-loss over iterations": {"iterations": iter_index+1, "w_i-loss": losses["w_i"].detach()},
                "w_t_i-loss over iterations": {"iterations": iter_index+1, "w_t_i-loss": losses["w_t_i"].detach()},
                "Q-loss over iterations": {"iterations": iter_index+1, "Q-loss": losses["Q"].detach()}}
    visualization.vis(vis_data)

    iter_index += 1
    if iter_index == maxiter:
        break

print("done")








torch.manual_seed(0) # ensure reproducibility

# ----------adaptation of the problem----------
q = lambda x: 1. # line load helps to identify E more reliably
w_ana =  lambda x: 1/24*(x**4 - 4*x**3 + 6*x**2) # exact analytical solution
dynamic = False
n_samples_t = 1
t_end = 0

# ----------setup of the model and optimizer----------
layer_widths_widths = [2, 15, 15, 2]
ffnn = FFNN(layer_widths_widths, nonlinearity="tanh", device="cuda:0")
print(ffnn.network)

# for some reason here RMSprop works better than Adam (with a sufficiently low learning rate)
#optimizer = torch.optim.Adam(params=ffnn.parameters(), lr=0.0005, weight_decay=0)
optimizer = torch.optim.RMSprop(params=ffnn.parameters(), lr=0.0005, weight_decay=0)
lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1)

# ----------setup of the visualization----------
visualization = Visualization(model=ffnn, l=l, t_end=t_end, plot_interval=50, dynamic=dynamic)

# ----------setup of the points, at which physics the loss will be evaluated----------
n_samples_t = 5 if dynamic else 1
n_samples_x = 5000
meshgrid = torch.meshgrid(torch.linspace(0, t_end, n_samples_t),
                           torch.linspace(0, l, n_samples_x), indexing="ij")
t = meshgrid[0].reshape(-1,1).type(ffnn.dtype).requires_grad_().to(ffnn.device)
x = meshgrid[1].reshape(-1,1).type(ffnn.dtype).requires_grad_().to(ffnn.device)

# ----------setup of the points, at which physics the loss will be evaluated (new samples with known values for w)----------
n_data = 5000
t_data = torch.zeros((n_data, 1), dtype=ffnn.dtype, device=ffnn.device)
x_data = torch.linspace(0., l, n_data, dtype=ffnn.dtype, device=ffnn.device).reshape(-1,1)

# ----------training loop----------
batch_size = 5000 # can be any value between 1 and n_samples_t*n_samples_x
maxiter = 100000 # maximmum number of iterations before the training is stopped        
iter_index = 0
scheduler_index = 0
while True:
    ffnn.zero_grad()
    ffnn.train()

    # retrieve batch
    t_i, x_i = get_batch([t, x], iter_index, batch_size, random_samples=True)

    # forward pass
    out = ffnn(torch.concatenate([t_i, x_i], dim=1))
    w = out[:,0].reshape(-1,1)
    E = out[:,1]
    E = E[torch.randperm(len(E))][0] # select a random element from E --> interpolation would be more general, but isn't supported by PyTorch out of the box

    # compute components of phsics loss
    losses = comp_loss(t_i, x_i, w, E=lambda x: E, I=I)

    # compute data loss
    data_indices = torch.randperm(len(t_data))[:int(len(t_data)/1)] # choose random batches from PDE data
    t_data_i = t_data[data_indices]
    x_data_i = x_data[data_indices]
    out_data = ffnn(torch.concatenate([t_data_i, x_data_i], dim=1))
    w_data = out_data[:,0].reshape(-1,1)
    losses["data"] = ((w_data - w_ana(x_data_i))**2).mean()
    # convergence can be sped up a lot by enforcing a constant value of E across the length of the beam (random sampling above has to be removed)
    #E_x = torch.autograd.grad(inputs=x, outputs=E, grad_outputs=torch.ones_like(E, device=x_data_i.device), create_graph=True, retain_graph=True)[0]
    #losses["E_x"] = E_x.mean()**2
    
    # weighting of loss components
    losses["pde"] = 1.*losses["pde"]
    losses["w"] = 1.*losses["w"]
    losses["w_x"] = 1.*losses["w_x"]
    losses["Q"] = 1.*losses["Q"]
    losses["M"] = 1.*losses["M"]
    losses["w_i"] = 1.*losses["w_i"]
    losses["w_t_i"] = 1.*losses["w_t_i"]
    losses["data"] = 1.*losses["data"]
    #losses["E_x"]  = 1.*losses["E_x"]

    loss_normalized = sum(losses.values())

    # backpropagation
    loss_normalized.backward()

    # optimizer step
    optimizer.step()

    # learning rate scheduling (one step per epoch)
    scheduler_index += batch_size
    if scheduler_index > len(t):
        scheduler_index = scheduler_index % len(t)
        lr_scheduler.step()

    # visualization
    if random.random() > 0.97:
        print(f"E: {E.mean().item():.2f}")
    losses = {key: value.to("cpu") for key, value in zip(losses.keys(), losses.values())}
    loss_normalized = loss_normalized.to("cpu")
    E = E.to("cpu")
    vis_data = {"total loss over iterations": {"iterations": iter_index+1, "total loss": loss_normalized.detach()},
                "learning rate over iterations": {"iterations": iter_index+1, "learning rate": lr_scheduler.get_last_lr()},
                "data-loss over iterations": {"iterations": iter_index+1, "data-loss": losses["data"].detach()},
                "PDE-loss over iterations": {"iterations": iter_index+1, "PDE-loss": losses["pde"].detach()},
                "w-loss over iterations": {"iterations": iter_index+1, "w-loss": losses["w"].detach()},
                "Q-loss over iterations": {"iterations": iter_index+1, "Q-loss": losses["Q"].detach()},
                "estimated E over iterations": {"iterations": iter_index+1, "estimated E": E.detach()}}
    visualization.vis(vis_data)

    iter_index += 1

    if iter_index == maxiter:
        break

print("done")















